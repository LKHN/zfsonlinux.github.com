Content-type: text/html

<HTML><HEAD><TITLE>Manpage of ZPOOLPROPS</TITLE>
</HEAD><BODY>
<H1>ZPOOLPROPS</H1>
Section: Maintenance Commands (8)<BR><A HREF="#index">Index</A>
<A HREF="/man/man2html">Return to Main Contents</A><HR>
<BR>BSD mandoc<BR>
<A NAME="lbAB">&nbsp;</A>
<H2>NAME</H2>



<B>zpoolprops</B>

 - available properties for ZFS storage pools

<A NAME="lbAC">&nbsp;</A>
<H2>DESCRIPTION</H2>

Each pool has several properties associated with it.
Some properties are read-only statistics while others are configurable and
change the behavior of the pool.
<P>

The following are read-only properties:
<DL COMPACT>
<P>

<DT><B>allocated</B>


<DD>
Amount of storage used within the pool.
See
<B>fragmentation</B>

and
<B>free</B>

for more information.
<DT><B>capacity</B>


<DD>
Percentage of pool space used.
This property can also be referred to by its shortened column name,
<B>cap</B>

 
<DT><B>expandsize</B>


<DD>
Amount of uninitialized space within the pool or device that can be used to
increase the total capacity of the pool.
On whole-disk vdevs, this is the space beyond the end of the GPT –
typically occurring when a LUN is dynamically expanded
or a disk replaced with a larger one.
On partition vdevs, this is the space appended to the partition after it was
added to the pool – most likely by resizing it in-place.
The space can be claimed for the pool by bringing it online with
<B>autoexpand=on</B>

or using
<B>zpool online -e</B>





 
<DT><B>fragmentation</B>


<DD>
The amount of fragmentation in the pool. As the amount of space
<B>allocated</B>

increases, it becomes more difficult to locate
<B>free</B>

space. This may result in lower write performance compared to pools with more
unfragmented free space.
<DT><B>free</B>


<DD>
The amount of free space available in the pool.
By contrast, the
<A HREF="/man/man2html?8+zfs">zfs</A>(8)


<B>available</B>

property describes how much new data can be written to ZFS filesystems/volumes.
The zpool
<B>free</B>

property is not generally useful for this purpose, and can be substantially more than the zfs
<B>available</B>

space. This discrepancy is due to several factors, including raidz parity; zfs
reservation, quota, refreservation, and refquota properties; and space set aside by
<B>spa_slop_shift</B>

(see
zfs-module-parameters5


for more information).
<DT><B>freeing</B>


<DD>
After a file system or snapshot is destroyed, the space it was using is
returned to the pool asynchronously.
<B>freeing</B>

is the amount of space remaining to be reclaimed.
Over time
<B>freeing</B>

will decrease while
<B>free</B>

increases.
<DT><B>health</B>


<DD>
The current health of the pool.
Health can be one of
<B>ONLINE , DEGRADED , FAULTED , OFFLINE, REMOVED , UNAVAIL</B>

 
<DT><B>guid</B>


<DD>
A unique identifier for the pool.
<DT><B>load_guid</B>


<DD>
A unique identifier for the pool.
Unlike the
<B>guid</B>

property, this identifier is generated every time we load the pool (e.g. does
not persist across imports/exports) and never changes while the pool is loaded
(even if a
<B>reguid</B>

operation takes place).
<DT><B>size</B>


<DD>
Total size of the storage pool.
<DT><B>unsupported@ </B><I>feature_guid</I>






<DD>
Information about unsupported features that are enabled on the pool.
See
zpool-features5


for details.
</DL>
<P>

<P>

The space usage properties report actual physical space available to the
storage pool.
The physical space can be different from the total amount of space that any
contained datasets can actually use.
The amount of space used in a raidz configuration depends on the characteristics
of the data being written.
In addition, ZFS reserves some space for internal accounting that the
<A HREF="/man/man2html?8+zfs">zfs</A>(8)


command takes into account, but the
<B></B>


command does not.
For non-full pools of a reasonable size, these effects should be invisible.
For small pools, or pools that are close to being completely full, these
discrepancies may become more noticeable.
<P>

The following property can be set at creation time and import time:
<DL COMPACT>
<P>

<DT><B>altroot</B>


<DD>
Alternate root directory.
If set, this directory is prepended to any mount points within the pool.
This can be used when examining an unknown pool where the mount points cannot be
trusted, or in an alternate boot environment, where the typical paths are not
valid.
<B>altroot</B>

is not a persistent property.
It is valid only while the system is up.
Setting
<B>altroot</B>

defaults to using
<B>cachefile = none</B>







 
though this may be overridden using an explicit setting.
</DL>
<P>

<P>

The following property can be set only at import time:
<DL COMPACT>
<P>

<DT><B>readonly = on | off</B>














<DD>
If set to
<B>on</B>

 
the pool will be imported in read-only mode.
This property can also be referred to by its shortened column name,
<B>rdonly</B>

 
</DL>
<P>

<P>

The following properties can be set at creation time and import time, and later
changed with the
<B>zpool set</B>



command:
<DL COMPACT>
<P>

<DT><B>ashift = ashift</B>








<DD>
Pool sector size exponent, to the power of
<B>2</B>

(internally referred to as
<B>ashift</B>

). Values from 9 to 16, inclusive, are valid; also, the
value 0 (the default) means to auto-detect using the kernel's block
layer and a ZFS internal exception list. I/O operations will be aligned
to the specified size boundaries. Additionally, the minimum (disk)
write size will be set to the specified size, so this represents a
space vs. performance trade-off. For optimal performance, the pool
sector size should be greater than or equal to the sector size of the
underlying disks. The typical case for setting this property is when
performance is important and the underlying disks use 4KiB sectors but
report 512B sectors to the OS (for compatibility reasons); in that
case, set
<B>ashift=12</B>

(which is 1&lt;&lt;12 = 4096). When set, this property is
used as the default hint value in subsequent vdev operations (add,
attach and replace). Changing this value will not modify any existing
vdev, not even on disk replacement; however it can be used, for
instance, to replace a dying 512B sectors disk with a newer 4KiB
sectors device: this will probably result in bad performance but at the
same time could prevent loss of data.
<DT><B>autoexpand = on | off</B>














<DD>
Controls automatic pool expansion when the underlying LUN is grown.
If set to
<B>on</B>

 
the pool will be resized according to the size of the expanded device.
If the device is part of a mirror or raidz then all devices within that
mirror/raidz group must be expanded before the new space is made available to
the pool.
The default behavior is
<B>off</B>

 
This property can also be referred to by its shortened column name,
<B>expand</B>

 
<DT><B>autoreplace = on | off</B>














<DD>
Controls automatic device replacement.
If set to
<B>off</B>

 
device replacement must be initiated by the administrator by using the
<B>zpool replace</B>



command.
If set to
<B>on</B>

 
any new device, found in the same physical location as a device that previously
belonged to the pool, is automatically formatted and replaced.
The default behavior is
<B>off</B>

 
This property can also be referred to by its shortened column name,
<B>replace</B>

 
Autoreplace can also be used with virtual disks (like device
mapper) provided that you use the /dev/disk/by-vdev paths setup by
vdev_id.conf. See the
vdev_id8


man page for more details.
Autoreplace and autoonline require the ZFS Event Daemon be configured and
running.  See the
<A HREF="/man/man2html?8+zed">zed</A>(8)


man page for more details.
<DT><B>autotrim = on | off</B>














<DD>
When set to
<B>on</B>

space which has been recently freed, and is no longer allocated by the pool,
will be periodically trimmed.  This allows block device vdevs which support
BLKDISCARD, such as SSDs, or file vdevs on which the underlying file system
supports hole-punching, to reclaim unused blocks.  The default setting for
this property is
<B>off</B>

 
<P>

Automatic TRIM does not immediately reclaim blocks after a free. Instead,
it will optimistically delay allowing smaller ranges to be aggregated in to
a few larger ones.  These can then be issued more efficiently to the storage.
TRIM on L2ARC devices is enabled by setting
<B>l2arc_trim_ahead &gt; 0</B>

 
<P>

Be aware that automatic trimming of recently freed data blocks can put
significant stress on the underlying storage devices.  This will vary
depending of how well the specific device handles these commands.  For
lower end devices it is often possible to achieve most of the benefits
of automatic trimming by running an on-demand (manual) TRIM periodically
using the
<B>zpool trim</B>



command.
<DT><B>bootfs = (unset) | </B><I>pool </I><B>/ </B><I>dataset</I>




















<DD>
Identifies the default bootable dataset for the root pool. This property is
expected to be set mainly by the installation and upgrade programs.
Not all Linux distribution boot processes use the bootfs property.
<DT><B>cachefile = </B><I>path </I><B>| none</B>














<DD>
Controls the location of where the pool configuration is cached.
Discovering all pools on system startup requires a cached copy of the
configuration data that is stored on the root file system.
All pools in this cache are automatically imported when the system boots.
Some environments, such as install and clustering, need to cache this
information in a different location so that pools are not automatically
imported.
Setting this property caches the pool configuration in a different location that
can later be imported with
<B>zpool import -c</B>





 
Setting it to the value
<B>none</B>

creates a temporary pool that is never cached, and the
Qq (empty string)



uses the default location.
<P>

Multiple pools can share the same cache file.
Because the kernel destroys and recreates this file when pools are added and
removed, care should be taken when attempting to access this file.
When the last pool using a
<B>cachefile</B>

is exported or destroyed, the file will be empty.
<DT><B>comment = </B><I>text</I>








<DD>
A text string consisting of printable ASCII characters that will be stored
such that it is available even if the pool becomes faulted.
An administrator can provide additional information about a pool using this
property.
<DT><B>dedupditto = </B><I>number</I>








<DD>
This property is deprecated and no longer has any effect.
<DT><B>delegation = on | off</B>














<DD>
Controls whether a non-privileged user is granted access based on the dataset
permissions defined on the dataset.
See
<A HREF="/man/man2html?8+zfs">zfs</A>(8)


for more information on ZFS delegated administration.
<DT><B>failmode = wait | continue | panic</B>




















<DD>
Controls the system behavior in the event of catastrophic pool failure.
This condition is typically a result of a loss of connectivity to the underlying
storage device(s) or a failure of all devices within the pool.
The behavior of such an event is determined as follows:
<DL COMPACT>
<P>

<DT><B>wait</B>


<DD>
Blocks all I/O access until the device connectivity is recovered and the errors
are cleared.
This is the default behavior.
<DT><B>continue</B>


<DD>
Returns
Er EIO

to any new write I/O requests but allows reads to any of the remaining healthy
devices.
Any write requests that have yet to be committed to disk would be blocked.
<DT><B>panic</B>


<DD>
Prints out a message to the console and generates a system crash dump.
</DL>
<P>

<DT><B>feature@ </B><I>feature_name </I><B>= enabled</B>












<DD>
The value of this property is the current state of
<I>feature_name</I>

 
The only valid value when setting this property is
<B>enabled</B>

which moves
<I>feature_name</I>

to the enabled state.
See
zpool-features5


for details on feature states.
<DT><B>listsnapshots = on | off</B>














<DD>
Controls whether information about snapshots associated with this pool is
output when
<B>zfs list</B>



is run without the
-<B>t</B>

option.
The default value is
<B>off</B>

 
This property can also be referred to by its shortened name,
<B>listsnaps</B>

 
<DT><B>multihost = on | off</B>














<DD>
Controls whether a pool activity check should be performed during
<B>zpool import</B>



 
When a pool is determined to be active it cannot be imported, even with the
-<B>f</B>

option.  This property is intended to be used in failover configurations
where multiple hosts have access to a pool on shared storage.
<P>

Multihost provides protection on import only.  It does not protect against an
individual device being used in multiple pools, regardless of the type of vdev.
See the discussion under
<B>zpool create.</B>

<P>

When this property is on, periodic writes to storage occur to show the pool is
in use.  See
<B>zfs_multihost_interval</B>

in the
zfs-module-parameters5


man page.  In order to enable this property each host must set a unique hostid.
See
<A HREF="/man/man2html?1+genhostid">genhostid</A>(1)


<A HREF="/man/man2html?8+zgenhostid">zgenhostid</A>(8)


spl-module-parameters5


for additional details.  The default value is
<B>off</B>

 
<DT><B>version = </B><I>version</I>








<DD>
The current on-disk version of the pool.
This can be increased, but never decreased.
The preferred method of updating pools is with the
<B>zpool upgrade</B>



command, though this property can be used when a specific version is needed for
backwards compatibility.
Once feature flags are enabled on a pool this property will no longer have a
value.
</DL>
<P>

<P>

<HR>
<A NAME="index">&nbsp;</A><H2>Index</H2>
<DL>
<DT><A HREF="#lbAB">NAME</A><DD>
<DT><A HREF="#lbAC">DESCRIPTION</A><DD>
</DL>
<HR>
This document was created by
<A HREF="/man/man2html">man2html</A>,
using the manual pages.<BR>
Time: 20:34:02 GMT, June 23, 2021
</BODY>
</HTML>
